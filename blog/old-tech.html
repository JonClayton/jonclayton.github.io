<!DOCTYPE html>
<html>
  <head>
    <title>The Big O--not quite what you may think</title>  
    <meta charset="UTF-8">
    <script src="https://cdn.rawgit.com/google/code-prettify/master/loader/run_prettify.js"></script>
    <link href='https://fonts.googleapis.com/css?family=Rambla:400,400italic,700' rel='stylesheet' type='text/css'>
    <link rel="stylesheet" type="text/css" href="../stylesheets/blog.css">

  <body>
    <header class="blog-header">
      <div class="blog-header">
      <h2>The Big O--not quite what you may think</h2>
    </header>

    <section class="blog-text">
          <p>Okay, I have to confess that when I saw the topic "The Big O", the first thought that crossed my mind was that the topic is NSFW.  Then shortly after came this guy:</p>
          <img alt= "Oscar Robertson signed card" src="https://c1.staticflickr.com/5/4011/4371181087_1bdc17e644_z.jpg?zz=1">
          <p>Oscar Robertson, nicknamed "The Big O", was the only player in NBA history to <strong>average</strong> a triple-double over a whole season (ten-plus points, rebounds and assists), and he actually averaged that over the first five seasons of his career. He was probably the greatest basketball player ever under 6'6". Like baseball's Curt Flood, he was also the plaintiff is a lawsuit that brought down restrictions on player movement and led to dramatic increases in player salaries. But I digress.</p>
          <p>But the first listed thing in Google for "Big O" isn't either la petite mort or an NBA Hall of Famer.  It's Big O's Golf and Big O's Crawfish here in Houston--dang Google knowing where I live!  Again, not what they are looking for. Time to try Wikipedia:</p>
          <img alt= "Big O Wikipedia screenshot" src="jonclayton.github.io/imgs/big-o-anime.png" width="90%" height="auto">
          <p>Not it, and far less interesting to me than the previous two Big O's. But the disambiguation link is promising, and after seeing a Shel Silverstein book <a href="http://www.amazon.com/Missing-Piece-Meets-Big/dp/0060256575">The Missing Piece Meets the Big O</a>, Barack Obama (who knew?!), the greek letter Omega (&Omega), and the world's largest centerless Ferris wheel, whatever that means, we come to what we're supposed to be discussing: "time complexity in comptuer science."</p>
          <p>This is actually a meaningful topic to me this week because while building my game last week (a working version of checkers found here: <a href="http://jonclayton.github.io/projects/checkers.html"></a>, I started to ponder the AI needed to make the computer moves. I started to think about recursively looking at the possible future moves, but it seemed clear the number of possible moves might expand so dramatically the computer couldn't handle them. A computer can easily "solve" tic-tac-toe by anticipating all possible outcomes. Not so for Go or chess, and probably a real challenge for checkers.  So does this Big O have something to do with that? Let's find out!</p>
          <p>Here's two relevant Wikipedia entries:</p>
          <ul><li><a href="https://en.wikipedia.org/wiki/Big_O_notation">Big O Notation</a></li>
            <li><a href="https://en.wikipedia.org/wiki/Analysis_of_algorithms">Analysis of Algorithms</a></li>
          </ul>
          <p>These articles are full of number theory and stuff that I'd only ask a math major to understand, much less explain. So here's a much more accessible article, written to help you be knowledgeable enough about Big O to make it through an interview: <a href="https://www.interviewcake.com/article/big-o-notation-time-and-space-complexity">Big O Notation: Using not-boring math to measure code's efficiency</a>. I think this is actually worth reading, it's fairly accessible, but I'm going to give a shot at a summary.</p>
          <h3>The Big (O) Finally!</h3>
          <p>Big O notation lets us express, in very simple terms, just how fast a function is growing. This is useful in computing because we often ask the computer to do things that take increasing amounts of time and/or memory as the list of inputs gets larger. First let's talk about the math and then we'll talk about some code.</p>
          <p>Big O is really a big simplification. The growth rate of a function, as the variable x gets very large, depends only on the part of the function that is growing the fastest.  Since x^4 grows faster than x^2 (dramatically so for large values of x), a function that includes both of them really only grows at the speed of the fastest growing element.  And coefficients don't matter either: 6*x^4 doesn't grow any faster than x^4. What Big O notation tells us to do is describe the growth rate of a function as just the simple version of its fastest growing component.  So the growth rate as x gets very large of 4*x^3+2*x^2+800000*x+32 is just O(x^3).  You'd say that function runs in Big O of x^3 time.</p>
          <p>If you don't just accept the math above, let's talk about it a little.  Growth rate can be expressed as the value for n+1 divided by the value for n (if you subtracted one from this you would have the percent growth, right?).  So if the function is just a*x+b, your growth would be (a*(n+1)+b)/(a*n+b).  As n gets very large, the constant 'b' won't matter much and the 'a' on top and bottom will just cancel out, so (n+1)/n is driving the growth.</p>
          <p>Why do we care?  If we have an algorithm that operates at a different Big O level, it may make a huge difference in the time it takes to solve the problem.  For example, if your sorting algorithm looks through your entire list of values to find the largest one, picks it out to put at the top of the sorted list, and then iterates through the list repeatedly until it gets all the numbers, its going to do 'n' trips through the list with n/2 average comparisons each time, so it will do (n^2)/2 operation and have a Big O of n-squared.  If you use quicksort (I hope someone else explains sort techniques), the average operations to completion is n*log(n), which is a Big O of just 'n'.  With a very long list you will save a huge amount of time.  In fact, at large enough numbers this is the different between the program running in a few seconds and taking so long that it's impractical to try to do.  As a result, for complex problems with massive datasets, the real challenge is an algorithm that reduces the Big O and makes the problem solvable.</p>
          <p>Thanks for coming to my blog.  Next week is a lot of travel and then I'll be on campus in San Francisco after Thanksgiving and really drinking from the coding firehose.</p>


  </body>
  </html>
